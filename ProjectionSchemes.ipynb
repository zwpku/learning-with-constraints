{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asdz7z3Pb7hA"
   },
   "source": [
    "## This notebook implements two constrained training scheme based on Newton's method.\n",
    "\n",
    "### Newton's method\n",
    "\n",
    "---\n",
    "\n",
    "### Given $g: \\mathbb{R}^N\\rightarrow \\mathbb{R}^k$, $x\\in \\mathbb{R}^N$, $v \\in \\mathbb{R}^{N\\times k}$ find $\\lambda\\in \\mathbb{R}^k$, such that  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "   g(x + \\tau v \\lambda) = 0 \n",
    "   \\end{aligned}\n",
    "$$\n",
    "\n",
    "Netwon's method:\n",
    "\n",
    "$$\n",
    "\\lambda_{n+1} = \\lambda_n - \\tau^{-1}(\\nabla g(x_n + \\tau v\\lambda_n) v)^{-1} g(x_n + \\tau v \\lambda_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hc6Gcie7-m_N"
   },
   "source": [
    "\n",
    "\n",
    "### First, load neceesary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bv_cHda_b7hC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import torch\n",
    "import math \n",
    "import torch.nn as nn\n",
    "import random\n",
    "import itertools \n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_projection(g, model, Y, v_direction, tau, res_tol=1e-3, max_newton_steps=100):       \n",
    "    iter_step = 0\n",
    "    converged = False\n",
    "    projection_iter_steps = 0 \n",
    "            \n",
    "    while iter_step < max_newton_steps:\n",
    "        g_now = g(model, Y)\n",
    "        \n",
    "        if iter_step == 0:\n",
    "            k = len(g_now)\n",
    "            lam = np.zeros((k))\n",
    "       # if self.verbose:\n",
    "        #print (f'projection step={iter_step}, g={g_now.detach().item():.3e}')\n",
    "            \n",
    "        if torch.linalg.norm(g_now) < res_tol:\n",
    "            converged = True\n",
    "            break\n",
    "\n",
    "        grad_g = [torch.autograd.grad(g_now[idx], model.parameters(), \n",
    "                                      create_graph=True, retain_graph=True, allow_unused=True) for idx in range(k)]\n",
    "               \n",
    "        with torch.no_grad():\n",
    "            mat = np.zeros((k, k))\n",
    "            for i in range(k):\n",
    "                for j in range(k):\n",
    "                    for z1, z2 in zip(grad_g[i], v_direction[j]):\n",
    "                        if z1 is not None: # and z2 is not None:\n",
    "                            mat[i,j] += (z1 * z2).sum()\n",
    "                            \n",
    "            dlam = -1.0 * np.linalg.solve(mat, g_now.numpy()) / tau\n",
    "            lam += dlam\n",
    "            for i in range(k):\n",
    "                for param, z2 in zip(model.parameters(), v_direction[i]):\n",
    "                    if z2 is not None:\n",
    "                        param.add_(z2 * dlam[i] * tau)\n",
    "                    \n",
    "        iter_step += 1\n",
    "    \n",
    "    return converged, iter_step, lam \n",
    "        \n",
    "class ProjectedSGD():\n",
    "    \n",
    "    def __init__(self, model, f, g, beta=1.0, tau=1e-1, res_tol=1e-3, max_newton_steps=100, verbose=True):\n",
    "        self.f = f\n",
    "        self.g = g\n",
    "        self.tau = tau\n",
    "        self.res_tol = res_tol\n",
    "        self.max_newton_steps = max_newton_steps\n",
    "        self.verbose = verbose\n",
    "        self.model = model\n",
    "        self.beta = beta\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        \n",
    "    def reset_data(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def unconstrained_update(self):\n",
    "        self.loss = self.f(self.model, self.X)\n",
    "        self._grad_f = torch.autograd.grad(self.loss, self.model.parameters(), allow_unused=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for param, grad in zip(self.model.parameters(), self._grad_f):\n",
    "                rn = torch.normal(torch.zeros(param.size()), torch.ones(param.size()))\n",
    "                if grad is not None:\n",
    "                    param.add_(-1.0 * grad * self.tau + math.sqrt(2.0 * self.tau / self.beta) * rn)       \n",
    "                else :\n",
    "                    param.add_(math.sqrt(2.0 * self.tau / self.beta) * rn)       \n",
    "                    \n",
    "    def projection_update(self):\n",
    "        g_val = self.g(self.model, self.Y)\n",
    "        grad_g_prev = [torch.autograd.grad(g_val[idx], self.model.parameters(), \n",
    "                                      create_graph=True, retain_graph=True, allow_unused=True) for idx in range(len(g_val))] \n",
    "        \n",
    "        self.converged, self.projection_iter_steps, lam = newton_projection(self.g, self.model, self.Y,\n",
    "                                                                            grad_g_prev, self.tau, \n",
    "                                                                            self.res_tol, self.max_newton_steps)\n",
    "        \n",
    "    def is_succeed(self):\n",
    "        return self.converged\n",
    "    \n",
    "    def projection_steps(self):\n",
    "        return self.projection_iter_steps\n",
    "\n",
    "    def step(self, X, Y):\n",
    "        self.reset_data(X, Y)\n",
    "        self.unconstrained_update()\n",
    "        self.projection_update()\n",
    "        \n",
    "class ProjectedLangevin():\n",
    "    def __init__(self, model, f, g, tau=0.1, beta=1.0, alpha=0.5, res_tol=1e-5, max_newton_steps=100):\n",
    "        self.model = model\n",
    "        self.tau = tau\n",
    "        self.beta = beta\n",
    "        self.alpha = alpha\n",
    "        self.res_tol = res_tol\n",
    "        self.max_newton_steps = max_newton_steps\n",
    "        self.p_list = []\n",
    "        self.g = g\n",
    "        self.f = f\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        \n",
    "        for param in model.parameters():\n",
    "            self.p_list.append(torch.zeros(param.size()))\n",
    "    \n",
    "    def reset_data(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "               \n",
    "    def unconstrained_update(self):\n",
    "        self.loss = self.f(self.model, self.X)\n",
    "        # momnentum update\n",
    "        grad_f = torch.autograd.grad(self.loss, self.model.parameters(), allow_unused=True)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for param, grad in zip (self.p_list, grad_f):\n",
    "                param.add_(-0.5 * self.tau * grad)\n",
    "\n",
    "            # position update\n",
    "            for param, param_p in zip (self.model.parameters(), self.p_list):\n",
    "                #print (param_p)\n",
    "                param.add_(self.tau * param_p)\n",
    "    \n",
    "    def projection_momentum(self):\n",
    "        g_val = self.g(self.model, self.Y)\n",
    "        k = len(g_val)\n",
    "        grad_g = [torch.autograd.grad(g_val[idx], self.model.parameters(), \n",
    "                                      create_graph=True, retain_graph=True, allow_unused=True) for idx in range(k)]\n",
    "        with torch.no_grad():\n",
    "            mat = np.zeros((k, k))\n",
    "            for i in range(k):\n",
    "                for j in range(k):\n",
    "                    for z1, z2 in zip(grad_g[i], grad_g[j]):\n",
    "                        mat[i,j] += (z1 * z2).sum()            \n",
    "                        \n",
    "            lam = np.zeros((k))\n",
    "            for idx in range(k):\n",
    "                for param, grad in zip (self.p_list, grad_g[idx]):        \n",
    "                    lam[idx] += (param * grad).sum()\n",
    "                    \n",
    "            coeff = np.linalg.solve(mat, lam)    \n",
    "                    \n",
    "            for idx in range(k):\n",
    "                for param, grad in zip (self.p_list, grad_g[idx]):        \n",
    "                    param.add_(-1.0 * coeff[idx] * grad)\n",
    "                                                    \n",
    "    def momentum_refresh(self):\n",
    "        for param in self.p_list:\n",
    "            rn = torch.normal(torch.zeros(param.size()), torch.ones(param.size()))\n",
    "            param.add_((self.alpha - 1.0) * param + math.sqrt((1-self.alpha**2)/self.beta) * rn)\n",
    "        self.projection_momentum()\n",
    "            \n",
    "    def projection(self):\n",
    "        g_val = self.g(self.model, self.Y)\n",
    "        k = len(g_val)\n",
    "        grad_g_prev = [torch.autograd.grad(g_val[idx], self.model.parameters(), \n",
    "                                      create_graph=True, retain_graph=True, allow_unused=True) for idx in range(k)] \n",
    "        \n",
    "        #project position, x -> x_{1}\n",
    "        converged, iter_steps, lam = newton_projection(self.g, self.model, self.Y, grad_g_prev, self.tau, \n",
    "                                                       self.res_tol, self.max_newton_steps)\n",
    "        \n",
    "        self.loss = self.f(self.model, self.X)\n",
    "        # momnentum update: p_{1/2}->p_1\n",
    "        grad_f = torch.autograd.grad(self.loss, self.model.parameters(), allow_unused=True)               \n",
    "                \n",
    "        with torch.no_grad():\n",
    "            # modify momentum to get p_{1/2}\n",
    "            for idx in range(k):\n",
    "                for param, grad in zip (self.p_list, grad_g_prev[idx]):\n",
    "                    param.add_(grad * lam[idx])\n",
    "\n",
    "            # momnentum update: p_{1/2}->p_1\n",
    "            for param, grad in zip (self.p_list, grad_f):\n",
    "                param.add_(-0.5 * self.tau * grad)\n",
    "        \n",
    "        self.projection_momentum()\n",
    "        \n",
    "        # p_1 -> p_{1,-}\n",
    "        for param in self.p_list:\n",
    "            param *= -1.0\n",
    "\n",
    "    def step(self, X, Y):\n",
    "        self.reset_data(X, Y)\n",
    "        self.momentum_refresh()\n",
    "        self.unconstrained_update()\n",
    "        self.projection()\n",
    "#        self.momentum_refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.1493,  1.1381], requires_grad=True)\n",
      "tensor([0.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from TestProblem import SimpleTest\n",
    "\n",
    "problem = SimpleTest(1.0)\n",
    "model = problem.create_model([1, 2.0])\n",
    "\n",
    "g_val = problem.g(model, None)\n",
    "grad_g_prev = [torch.autograd.grad(g_val[idx], model.parameters(), \n",
    "                                      create_graph=True, retain_graph=True, allow_unused=True) for idx in range(len(g_val))] \n",
    "        \n",
    "newton_projection(problem.g, model, None, grad_g_prev, tau=0.1, res_tol=1e-10)\n",
    "\n",
    "print (model.x)\n",
    "print (problem.g(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=0, loss=3.0\n",
      "step=100, loss=0.5481491684913635\n",
      "step=200, loss=-0.45217540860176086\n",
      "step=300, loss=-0.6795360445976257\n",
      "step=400, loss=-0.9348873496055603\n",
      "step=500, loss=-0.823089599609375\n",
      "step=600, loss=-0.780804455280304\n",
      "step=700, loss=-0.8840094804763794\n",
      "step=800, loss=-0.9682284593582153\n",
      "step=900, loss=-0.9883614182472229\n",
      "[-0.2841745  -0.67461526]\n"
     ]
    }
   ],
   "source": [
    "from TestProblem import SimpleTest\n",
    "\n",
    "problem = SimpleTest(1.0)\n",
    "model = problem.create_model([1, 2.0])\n",
    "\n",
    "opt = ProjectedSGD(model, problem.f, problem.g, tau=0.01, beta=20.0, verbose=True)\n",
    "#opt = ProjectedLangevin(model, simple_f, simple_g, tau=0.3, beta=8.0, alpha=0.5)\n",
    "\n",
    "n_steps = 1000\n",
    "for idx in range(n_steps):\n",
    "    opt.step(None, None)\n",
    "    if idx % 100 == 0:\n",
    "        print (f'step={idx}, loss={opt.loss.detach().numpy()}')\n",
    "    #print (model.x.detach().numpy())\n",
    "print (model.x.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "poisson2d.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
